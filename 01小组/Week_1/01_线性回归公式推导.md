# 多元线性回归

[TOC]



## 定义

在学习多元线性回归之前，首先了解其定义

【百度词条相关介绍】**在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。**

引入一个例子(引自CSDN)eg.1

| 房屋面积(m^2) | 租凭价格(x1000) |
| ------------- | --------------- |
| 10            | 0.8             |
| 15            | 1.0             |
| 20            | 1.8             |
| 30            | 2               |
| 50            | 3.2             |

对于上述数据集，我们通过分析可以得出价格与面积呈线性增长的关系，我们可以用一条直线来拟合所有的样本点，即是
```math
y=ax+b
```
那么，当数据集维度增加时，我们同样可以对数据进行分析，用一个超平面来拟合所有的样本点，例如eg.2

| Number | Tv    | Radio | Newspaper | Sales |
| ------ | ----- | ----- | --------- | ----- |
| 1      | 230.1 | 37.8  | 69.2      | 22.1  |
| 2      | 44.5  | 39.3  | 45.1      | 10.4  |
| 3      | 17.2  | 45.9  | 69.3      | 9.3   |
| 4      | 151.5 | 41.3  | 58.5      | 18.5  |
| 5      | 180.8 | 10.8  | 58.4      | 12.9  |

对于该数据集，我们可以用一个超平面来拟合所有的样本点，即
```math
h(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}
```
那么推广至二元及二元以上的自变量，我们便得到了多元线性回归模型，也可以写成矩阵相乘的形式
```math
h(x)=\sum_{i=0}^{N} \theta_{i}x_{i}=\theta^{T}X(其中N表示自变量的数量)
```

于是，对于每一个样本点都有：

```math
y_{i}=\theta^{T}x_{i}+\epsilon_{i}
```

其中，公式的第一项$`\theta^{T}x_{i}`$表示的是预测值$`\hat{y}`$，而$`\epsilon_{i}`$表示的是误差值，由于假设样本之间是相互独立的，那么误差变量随机产生，所以服从正态分布，于是我们假设$`\epsilon`$服从均值为0，方差为$`\sigma^{2}`$的高斯分布。



## 解析解推导

根据正态分布的概率密度函数我们可以得到：

```math
p(\epsilon_{i})=\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(\epsilon_{i})^{2}}{2\sigma^{2}})} 
```

将y代入函数得到：

```math
p(y_{i}|x_{i};\theta )=\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})} 
```

将各项联乘，我们得到了关于$`\theta`$的似然函数：

```math
L(\theta)=\prod_{i=1}^{m}\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})}  
```

将联乘变换成累加：

```math
ln(L(\theta))=ln(\prod_{i=1}^{m}\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})})
```

```math
=\sum_{i=1}^{m}ln(\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})})
```

```math
=m\times ln(\frac{1}{\sigma \sqrt{2\pi}})-\frac{1}{2\sigma ^{2}}\sum_{i=1}^{m}(y_{i}-\theta^{T}x_{i})^{2}
```

要让似然函数L取最大值，观察函数

```math
J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(y_{i}-\theta^{T}x_{i})^{2}
```

我们需要求得函数J的最小值，而当J取最小值时，满足条件：

```math
\frac{\partial J(\theta)}{\partial \theta} =0
```

即其导数值为0，对函数$`J(\theta)`$的自变量$`\theta`$求导，于是我们得到：

```math
J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(y_{i}-\theta^{T}x_{i})^{2}
```

```math
=\frac{1}{2}(X\theta-Y)^{T}(X\theta-Y)······(写成矩阵表达式)
```

```math
=\frac{1}{2}(\theta^{T}X^{T}-Y^{T})(X\theta-Y)
```

```math
=\frac{1}{2}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}Y-Y^{T}X\theta+Y^{T}Y)
```

```math
于是\frac{\partial J(\theta)}{\partial \theta} =\frac{1}{2}(2X^{T}X\theta-X^{T}Y-Y^{T}X)=0
```

```math
X^{T}X\theta=X^{T}Y
```

```math
得到解析解为：\theta=(X^{T}X)^{-1}X^{T}Y
```

## 正则化最小二乘法

在上一例的给出的线性模型中，我们定义拟合函数为：

```math
h(x)=\sum_{i=0}^{N} \theta_{i}x_{i}=\theta^{T}X(其中N表示自变量的数量)
```

但是该模型不能合理的完全拟合所有类型的数据集，因此我们将其扩充至任意阶的多项式，函数可以更一般的表示为：

```math
h(x)=\sum_{i=0}^{N} \theta_{i}x^{i}(其中N表示自变量的数量)
```

**N**称为该多项式函数的阶。

## 梯度下降法

当样本维度高，数据较多，计算机较难求解，我们可以通过梯度下降法来求解。梯度下降就是通过迭代让偏导数下降到最低，让损失函数$`J(\theta)`$最小。

在由损失函数所构成的平面中，我们可以初始化一个点作为起点，计算其梯度，朝梯度相反的方向移动，逐次迭代，直到达到终止条件，其中，终止条件可以是迭代次数，或是某一个较小的阀值。

用数学公式表示梯度下降即是：

```math
\Theta_{k+1}=\Theta_{k}-\alpha ·g
```

公式中的$`\Theta`$表示第k次迭代时的坐标，$`\alpha`$表示的是步长/学习率，需要设置合理的学习率才能达到理想效果，关于学习率对模型的学习效果可见“[参数设置与学习曲线实验结果](https://github.com/kingdomye/QG_AI/tree/master/code/py手搓神经网络/output)”，其中有详细的实验结果，而g表示的是梯度，取负号表示朝其反方向移动。

梯度即是前面所求：

```math
g=\frac{\partial J(\theta)}{\partial \theta} =X^{T}X\theta-X^{T}Y=X^{T}(X\theta-Y)
```

我们将其代入公式：

```math
\Theta_{k+1}=\Theta_{k}-\alpha ·X^{T}(X\theta-Y)
```

**算法过程：**

1、初始化$`\theta`$的值，得到初始时的梯度

2、更新梯度，逐步迭代

3、达到终止条件，得到结果$`\Theta_{k}`$

## 代码实现及数据可视化

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


class LinearRegression:
    def __init__(self):
        self.data = None
        self.X = None
        self.Y = None

    def read_data(self, filename='./data/boston.csv'):
        self.data = pd.read_csv(filename)

    def divide_dataset(self):
        self.X = self.data.iloc[:, :-1]
        self.Y = self.data.iloc[:, -1]

    def normalize(self):
        """归一化：将数据缩放到 [0, 1] 范围内"""
        X_min = np.min(self.X, axis=0)  # 每列的最小值
        X_max = np.max(self.X, axis=0)  # 每列的最大值
        self.X = (self.X - X_min) / (X_max - X_min)

    def standardize(self):
        """标准化：将数据缩放到均值为 0，标准差为 1 的分布"""
        mean = np.mean(self.X, axis=0)  # 每列的均值
        std = np.std(self.X, axis=0)  # 每列的标准差
        self.X = (self.X - mean) / std

    # 解析解法
    def solution_one(self):
        X = self.X.to_numpy()
        Y = self.Y.to_numpy()
        theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)
        print(theta)

    # 梯度下降法
    def solution_two(self, alpha=0.0001):
        X = self.X.to_numpy()
        Y = self.Y.to_numpy()

        theta = np.zeros(X.shape[1])
        for _ in range(1000):
            grad = X.T.dot(np.dot(X, theta) - Y)
            theta = theta - alpha * grad
        print(theta)
        return theta

    # 预测数据可视化
    def visualize_features(self):
        """绘制每个特征与目标值的关系图"""
        features = self.X.columns
        num_features = len(features)
        fig, axes = plt.subplots(nrows=num_features, figsize=(10, 5 * num_features))

        for i, feature in enumerate(features):
            axes[i].scatter(self.X[feature], self.Y, alpha=0.5)
            axes[i].set_title(f"{feature} && Target")
            axes[i].set_xlabel(feature)
            axes[i].set_ylabel("Target")

        plt.tight_layout()
        plt.show()

    def visualize_predictions(self, theta):
        """绘制预测结果与真实值的对比图"""
        X = self.X.to_numpy()
        Y_pred = X.dot(theta)
        plt.figure(figsize=(10, 6))
        plt.scatter(range(len(self.Y)), self.Y, label="True Values", alpha=0.5)
        plt.scatter(range(len(Y_pred)), Y_pred, label="Predictions", alpha=0.5, color="red")
        plt.xlabel("Index")
        plt.ylabel("Value")
        plt.title("True && Predictions")
        plt.legend()
        plt.show()


if __name__ == '__main__':
    # model training part
    linear_regression = LinearRegression()
    linear_regression.read_data()
    linear_regression.divide_dataset()
    linear_regression.normalize()
    theta = linear_regression.solution_two()

    # 可视化特征与目标值的关系
    linear_regression.visualize_features()

    # 可视化预测结果与真实值的对比
    linear_regression.visualize_predictions(theta)
```

