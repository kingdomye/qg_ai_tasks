{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:20.607349Z",
     "start_time": "2025-03-26T12:08:29.488382Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5845314886555997\n",
      "Epoch 2, Loss: 0.30062147266447925\n",
      "Epoch 3, Loss: 0.25347521135420686\n",
      "Epoch 4, Loss: 0.2211914599887026\n",
      "Epoch 5, Loss: 0.19599560045722578\n",
      "Epoch 6, Loss: 0.17581279328795893\n",
      "Epoch 7, Loss: 0.15958351554916994\n",
      "Epoch 8, Loss: 0.14600399256086172\n",
      "Epoch 9, Loss: 0.13501612401640872\n",
      "Epoch 10, Loss: 0.12541272491415237\n",
      "Accuracy without DP: 0.9627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.700052529509896, Epsilon: 0.14\n",
      "Epoch 2, Loss: 0.9080849973314098, Epsilon: 0.19\n",
      "Epoch 3, Loss: 0.6383176138406115, Epsilon: 0.23\n",
      "Epoch 4, Loss: 0.5202780249816522, Epsilon: 0.27\n",
      "Epoch 5, Loss: 0.47317902967810377, Epsilon: 0.30\n",
      "Epoch 6, Loss: 0.445561758196875, Epsilon: 0.33\n",
      "Epoch 7, Loss: 0.4321227859951921, Epsilon: 0.36\n",
      "Epoch 8, Loss: 0.4338335406694458, Epsilon: 0.38\n",
      "Epoch 9, Loss: 0.4244533451492471, Epsilon: 0.40\n",
      "Epoch 10, Loss: 0.4226146987887588, Epsilon: 0.43\n",
      "Accuracy with DP: 0.8916\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import struct\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from opacus import PrivacyEngine\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 自定义 MNIST 数据集加载器，支持读取 .gz 文件（Written by KIMI）\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        with gzip.open(labels_path, 'rb') as lbpath:\n",
    "            magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "            self.labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "        with gzip.open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "            self.images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(self.labels), 28, 28)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# 定义简单的神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 计算准确率的函数\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载本地 MNIST 数据集\n",
    "train_dataset = MNISTDataset(\n",
    "    images_path='./mnist_handwriting_data/train-images-idx3-ubyte.gz',\n",
    "    labels_path='./mnist_handwriting_data/train-labels-idx1-ubyte.gz',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = MNISTDataset(\n",
    "    images_path='./mnist_handwriting_data/t10k-images-idx3-ubyte.gz',\n",
    "    labels_path='./mnist_handwriting_data/t10k-labels-idx1-ubyte.gz',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 不使用差分隐私的模型\n",
    "model_no_dp = Net()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer_no_dp = optim.SGD(model_no_dp.parameters(), lr=0.01)\n",
    "\n",
    "# 训练不使用差分隐私的模型\n",
    "for epoch in range(10):\n",
    "    model_no_dp.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        optimizer_no_dp.zero_grad()\n",
    "        output = model_no_dp(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_no_dp.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# 计算不使用差分隐私的模型的准确率\n",
    "accuracy_no_dp = calculate_accuracy(model_no_dp, test_loader)\n",
    "print(f\"Accuracy without DP: {accuracy_no_dp:.4f}\")\n",
    "\n",
    "# 使用差分隐私的模型\n",
    "model_dp = Net()\n",
    "optimizer_dp = optim.SGD(model_dp.parameters(), lr=0.01)\n",
    "\n",
    "# 使用 PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model_dp, optimizer_dp, train_loader_dp = privacy_engine.make_private(\n",
    "    module=model_dp,\n",
    "    optimizer=optimizer_dp,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# 训练使用差分隐私的模型\n",
    "for epoch in range(10):\n",
    "    model_dp.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader_dp):\n",
    "        optimizer_dp.zero_grad()\n",
    "        output = model_dp(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_dp.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader_dp)}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# 计算使用差分隐私的模型的准确率\n",
    "accuracy_dp = calculate_accuracy(model_dp, test_loader)\n",
    "print(f\"Accuracy with DP: {accuracy_dp:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
