# é€»è¾‘å›å½’åŠå…¶ä»£ç 

[TOC]

## é€»è¾‘å›å½’ä»‹ç»

é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰æ˜¯ä¸€ç§ç»å…¸çš„åˆ†ç±»ç®—æ³•ï¼Œå°½ç®¡åå­—ä¸­æœ‰â€œå›å½’â€äºŒå­—ï¼Œä½†å®ƒä¸»è¦ç”¨äºè§£å†³**äºŒåˆ†ç±»é—®é¢˜**ï¼Œå³é¢„æµ‹çš„ç›®æ ‡å˜é‡æ˜¯äºŒå…ƒçš„ï¼ˆå¦‚0æˆ–1ã€æ˜¯æˆ–å¦ã€æ­£ç±»æˆ–è´Ÿç±»ï¼‰ã€‚é€»è¾‘å›å½’é€šè¿‡é€»è¾‘å‡½æ•°ï¼ˆSigmoidå‡½æ•°ï¼‰å°†çº¿æ€§å›å½’çš„è¾“å‡ºæ˜ å°„åˆ°(0, 1)åŒºé—´ï¼Œä»è€Œå°†é—®é¢˜è½¬åŒ–ä¸ºæ¦‚ç‡é¢„æµ‹ã€‚

**ã€é€»è¾‘å›å½’æ¨¡å‹ã€‘**

```math
P(y=1|x)=\frac{1}{1+e^{-\mathbf{ \omega ^{T}x}}} 
```

å…¶ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªsigmoidå‡½æ•°ï¼Œå¯ä»¥è¾“å‡º(0,1)åŒºé—´çš„æ•°å€¼ï¼Œè¡¨ç¤ºæŸä¸€ç»„ç‰¹å¾å€¼å±äºæŸä¸€ç±»çš„æ¦‚ç‡ï¼Œ$`\omega`$è¡¨ç¤ºçš„æ˜¯æ¨¡å‹çš„å‚æ•°çŸ©é˜µï¼Œ$`x`$è¡¨ç¤ºçš„æ˜¯æ¨¡å‹çš„ç‰¹å¾å€¼çŸ©é˜µï¼Œè€ŒPè¡¨ç¤ºçš„æ˜¯æ¨¡å‹åœ¨ç»™å®šç‰¹å¾xçš„æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹å±äºæ­£ç±»(y=1)çš„æ¦‚ç‡ï¼›

sigmoidå‡½æ•°çš„å¤§è‡´å›¾åƒå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°å®ƒè¾“å‡º(0,1)åŒºé—´çš„æ•°å€¼ï¼Œé€‚åˆè¡¨ç¤ºæ¦‚ç‡ï¼Œé€šè¿‡æ”¹å˜xçš„ç³»æ•°å¯ä»¥æ”¹å˜å›¾åƒçš„â€œå®½åº¦â€ï¼›

<img src="./imgs/001.png" alt="001" style="zoom: 67%;" />

**ã€æ¨¡å‹æŸå¤±å‡½æ•°ã€‘**

é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°æ˜¯å¯¹æ•°æŸå¤±ï¼Œä¹Ÿè¢«ç§°ä¸º**äº¤å‰ç†µæŸå¤±**ï¼Œè¡¨ç¤ºä¸ºï¼š

```math
L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right]
```

å…¶ä¸­ï¼Œ$`y_{i}`$è¡¨ç¤ºçœŸå®æ ‡ç­¾ï¼ˆ0/1ï¼‰ï¼Œ$`\hat{p_{i}} `$è¡¨ç¤ºæ¨¡å‹é¢„æµ‹æ ·æœ¬å±äºæ­£ç±»çš„æ¦‚ç‡ï¼ŒNæ˜¯æ ·æœ¬æ•°é‡ï¼›

è¯¥æŸå¤±å‡½æ•°ç”±ä¼¼ç„¶å‡½æ•°æ¨å¯¼è€Œæ¥ï¼Œä¼¼ç„¶å‡½æ•°ï¼š

```math
L(\omega )=\prod_{i=1}^{N}([p(x_{i})]^{y_{i}}[1-p(x_{i})]^{1-y_{i}}) 
```

è¦å°†å‡½æ•°çš„å¹‚è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†ç´¯ä¹˜å˜ä¸ºç´¯åŠ ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å‡½æ•°ä¸¤è¾¹å–å¯¹æ•°ï¼Œå¾—åˆ°ï¼š

```math
L(\omega )=\sum_{i=1}^{N} [y_{i}logp(x_{i})+(1-y_{i})log(1-p(x_{i}))]
```

**æ±‚ä¼¼ç„¶å‡½æ•°çš„æœ€å¤§å€¼å³æ˜¯æ±‚æŸå¤±å‡½æ•°çš„æœ€å°å€¼**ï¼Œæ‰€ä»¥ç»™å…¶æ·»åŠ è´Ÿå·ï¼ŒåŒæ—¶é˜²æ­¢ç´¯åŠ å€¼è¿‡å¤§ï¼Œå°†å…¶é™¤ä»¥Nï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°æŸå¤±å‡½æ•°ï¼š

```math
J(\omega)=-\frac{1}{N}logL(\omega)= -\frac{1}{N} \sum_{i=1}^{N} [y_{i}logp(x_{i})+(1-y_{i})log(1-p(x_{i}))]
```

 

## æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£(çº¯æ‰‹æ‰“å…¬å¼ğŸ¥µç»™ä¸ªstaræ”¯æŒä¸€ä¸‹ä¸»æ’­)

ç±»ä¼¼äºçº¿æ€§å›å½’ä¸­çš„æ¢¯åº¦ä¸‹é™æ³•(å¯è§[çº¿æ€§å›å½’å…¬å¼æ¨å¯¼](./W1_T2_çº¿æ€§å›å½’å…¬å¼æ¨å¯¼.md))ï¼Œé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™æ³•ä¹Ÿæ˜¯é€šè¿‡æ±‚æŸå¤±å‡½æ•°å…³äºå‚æ•°$`\omega`$çš„ä¸€é˜¶åå¯¼æ•°å¯»æ‰¾ä¸‹é™æ–¹å‘ï¼Œä»è€Œæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œæ¢¯åº¦è®¡ç®—ä¸ºï¼š

```math
\mathbf{grad} =\frac{\partial J(\omega )}{\partial \omega} =-\frac{1}{N}\sum [\frac{y_{i}}{p(x_{i})}Â·\frac{\partial p(x_{i})}{\partial \omega}-\frac{1-y_{i}}{1-p(x_{i})}Â·\frac{\partial p(x_{i})}{\partial \omega }  ] 
```

```math
=-\frac{1}{N} \sum \frac{\partial p(x_{i})}{\partial \omega }Â·[\frac{y_{i}}{p(x_{i})}-\frac{1-y_{i}}{1-p(x_{i})}]
```

```math
=-\frac{1}{N} \sum [p(x_{i})Â·(1-p(x_{i}))Â·x_{i}]Â·[\frac{y_{i}}{p(x_{i})}-\frac{1-y_{i}}{1-p(x_{i})}]
```

```math
åŒ–ç®€å¾—åˆ°ï¼š\mathbf{grad=\frac{\sum_{i=0}^{N} (p(x_{i})-y_{i})Â·x_{i}}{N} } 
```

é‚£ä¹ˆé€šè¿‡å¯¹å‚æ•°$`\omega `$è¿›è¡Œè¿­ä»£å°±èƒ½å¾—åˆ°ï¼š

```math
\omega_{i}^{k+1}= \omega_{i}^{k}-\alpha Â·\mathbf{grad} 
```

åŒæ ·çš„ï¼Œå…¶ä¸­$`\alpha`$è¡¨ç¤ºçš„æ˜¯æ¨¡å‹çš„å­¦ä¹ ç‡ï¼Œkè¡¨ç¤ºçš„æ˜¯è¿­ä»£çš„æ¬¡æ•°

## ä»£ç å®ç°

é¦–å…ˆå¯¼å…¥å¿…è¦çš„åº“ï¼Œåˆ†åˆ«ç”¨äºæ•°æ®è¯»å–ã€æ•°æ®å¤„ç†åŠå›¾åƒç»˜åˆ¶

```python
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
```

æ•°æ®å¤„ç†ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å½’ä¸€åŒ–ï¼Œå°†æ•°æ®ç¼©æ”¾åˆ°[0,1]ä¹‹é—´çš„æ•°å€¼ï¼š

```python
def normalize(X):
    """å½’ä¸€åŒ–ï¼šå°†æ•°æ®ç¼©æ”¾åˆ° [0, 1] èŒƒå›´å†…"""
    X_min = np.min(X, axis=0)  # æ¯åˆ—çš„æœ€å°å€¼
    X_max = np.max(X, axis=0)  # æ¯åˆ—çš„æœ€å¤§å€¼
    X = (X - X_min) / (X_max - X_min)
    return X
```

æ ¹æ®å‰é¢ä»‹ç»çš„é€»è¾‘å›å½’æ¨¡å‹ï¼Œå€ŸåŠ©numpyåº“æˆ‘ä»¬å¯ä»¥å¾—åˆ°sigmoidå‡½æ•°ï¼š

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

åŸºæœ¬çš„ä»£ç å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¼–å†™é€»è¾‘å›å½’ç±»ï¼Œå¯¹è±¡çš„å±æ€§ç”±**æ¨¡å‹è¾“å…¥Xã€è¾“å‡ºyåŠæ¨¡å‹å‚æ•°w**æ„æˆï¼š

```python
class LogisticRegression:
    def __init__(self):
        self.w = None
        self.X = None
        self.y = None
```

å°†æ•°æ®çš„è¯»å–å’Œæ•°æ®é›†çš„åˆ’åˆ†ä¹Ÿåˆ†è£…åˆ°ç±»ä¸­ï¼Œä»£ç è¾ƒç®€å•ï¼Œä¸å†èµ˜è¿°ï¼Œè¿™é‡Œé‡‡ç”¨çš„æ•°æ®é›†æ˜¯irisæ•°æ®é›†çˆ†æ”¹ç‰ˆï¼Œæ•°æ®é›†å…±åŒ…å«4ä¸ªç‰¹å¾å€¼ï¼Œ2ç§åˆ†ç±»ï¼š

```python
    def read_data(self, filename='./data/iris.csv'):
        """è¯»å–æ•°æ®"""
        df = pd.read_csv(filename)
        self.X = df.iloc[:, 1:-1].values
        self.y = df.iloc[:, -1].values

    def divide_data(self):
        """åˆ’åˆ†æ•°æ®é›†ï¼šè®­ç»ƒé›†/æµ‹è¯•é›†"""
        indices = np.arange(0, self.X.shape[0])
        np.random.shuffle(indices)

        train_size = int(self.X.shape[0] * 0.8)
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

        train_X = self.X[train_indices]
        train_y = self.y[train_indices]
        test_X = self.X[test_indices]
        test_y = self.y[test_indices]

        return train_X, train_y, test_X, test_y
```

æ¥ä¸‹æ¥æ˜¯å…³é”®çš„wæ¢¯åº¦ä¸‹é™æ±‚è§£ä»£ç ï¼Œé¦–å…ˆåˆå§‹åŒ–wï¼Œæ ¹æ®å‰é¢æ¨å‡ºçš„æ¢¯åº¦å…¬å¼ï¼Œå¯¹wè¿›è¡Œè¿­ä»£ï¼Œåœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼Œè¿™é‡ŒåŒæ—¶å¯¹æŸå¤±å‡½æ•°è¿›è¡Œè®¡ç®—å¹¶å­˜å…¥loss_historyä¸­ï¼Œç”¨äºæœ€åç»˜åˆ¶æŸå¤±å‡½æ•°å›¾åƒ(MSE LINE)ï¼Œæœ‰äº†å‰é¢çš„æ¨å¯¼åŸºç¡€ï¼Œä»£ç çš„å®ç°ä¹Ÿè¾ƒä¸ºç®€å•ï¼š

```python
    def train(self, epochs=100, learning_rate=0.01):
        """æ¢¯åº¦ä¸‹é™æ³•è¿­ä»£æ±‚è§£å‚æ•°w"""
        self.w = np.ones(self.X.shape[1])
        train_X, train_y, _, _ = self.divide_data()
        train_X = normalize(train_X)
        loss_history = []

        for epoch in range(epochs):
            # è®¡ç®—æ¢¯åº¦
            grad = np.mean(
                [((sigmoid(np.dot(self.w.T, train_X[i])) - train_y[i]) * train_X[i]) for i in range(train_X.shape[0])],
                axis=0)
            self.w -= learning_rate * grad

            # è®¡ç®—æŸå¤±
            loss = -np.mean([
                (train_y[i] * np.log(sigmoid(np.dot(self.w.T, train_X[i]))) +
                 (1 - train_y[i]) * np.log(1 - sigmoid(np.dot(self.w.T, train_X[i]))))
                for i in range(train_X.shape[0])
            ])
            loss_history.append(loss)

            if (epoch + 1) % 50 == 0:
                print('epoch: {}, loss: {}, w: {}'.format(epoch+1, loss, self.w))

        return loss_history
```

å®Œæˆè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å‰é¢åˆ’åˆ†å‡ºæ¥çš„æµ‹è¯•é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå‡†ç¡®ç‡è¯„ä¼°ï¼š

```python
    def evaluate(self):
        """æ¨¡å‹è¯„ä¼°"""
        accuracy = 0
        _, _, test_X, test_y = self.divide_data()
        for i in range(test_X.shape[0]):
            result = sigmoid(np.dot(self.w.T, test_X[i]))
            result = 0 if result < 0.5 else 1

            if result == test_y[i]:
                accuracy += 1

        accuracy /= test_X.shape[0]
        return accuracy
```

ä¸»å‡½æ•°ï¼š

```python
if __name__ == '__main__':
    lr = LogisticRegression()
    lr.read_data()
    lr_loss = lr.train(epochs=1000, learning_rate=0.1)

    # æ ¹æ®å‡†ç¡®ç‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°
    acc = lr.evaluate()
    print("Accuracy: {}%".format(acc * 100))

    # ç»˜åˆ¶æŸå¤±å‡½æ•°
    plt.title("MSE LINE")
    plt.plot(lr_loss)
    plt.show()
```

### å®éªŒç»“æœæŠ¥å‘Š

è§‚å¯Ÿè¿è¡Œä»£ç çš„è¾“å‡ºï¼š

```
epoch: 50, loss: 0.520230477198475
epoch: 100, loss: 0.4252965149805779
epoch: 150, loss: 0.35966419103464375
epoch: 200, loss: 0.3115685955385885
epoch: 250, loss: 0.2747885092279347
epoch: 300, loss: 0.24580201199535412
epoch: 350, loss: 0.22242564486841637
epoch: 400, loss: 0.20321559792144867
epoch: 450, loss: 0.18717587435011268
epoch: 500, loss: 0.1735981751098445
epoch: 550, loss: 0.1619664292929578
epoch: 600, loss: 0.15189677901579315
epoch: 650, loss: 0.14309847266680148
epoch: 700, loss: 0.13534767697042743
epoch: 750, loss: 0.1284695373632668
epoch: 800, loss: 0.12232564164193578
epoch: 850, loss: 0.11680510101371802
epoch: 900, loss: 0.11181810013720181
epoch: 950, loss: 0.1072911621315579
epoch: 1000, loss: 0.10316362415867512
Accuracy: 100.0%
```

æˆ‘ä»¬å¯ä»¥å‘ç°æ¨¡å‹å¯¹æµ‹è¯•é›†çš„æ‰€æœ‰æ•°æ®éƒ½æœ‰æå¥½çš„é¢„æµ‹ï¼Œæœ€ç»ˆçš„å‡†ç¡®ç‡ä¸ºAcc=100%ï¼Œè§‚å¯ŸæŸå¤±å›¾åƒï¼š

<img src="/Users/chenyingrui/Desktop/Code Learn/qg_ai/QG_AI_TASK/qg_ai_tasks/note/imgs/002.png" alt="002" style="zoom: 50%;" />

ä¸éš¾å‘ç°ï¼ŒæŸå¤±å‡½æ•°å›¾åƒæ˜¯ä¸€æ¡è‚˜å‹å…‰æ»‘æ›²çº¿ï¼ŒæŸå¤±é€’å‡ã€‚



Written by ricckker

2025/3/19 å¹¿ä¸œÂ·å¹¿å·