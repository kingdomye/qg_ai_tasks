# 逻辑回归及其代码

[TOC]

## 逻辑回归介绍

逻辑回归（Logistic Regression）是一种经典的分类算法，尽管名字中有“回归”二字，但它主要用于解决**二分类问题**，即预测的目标变量是二元的（如0或1、是或否、正类或负类）。逻辑回归通过逻辑函数（Sigmoid函数）将线性回归的输出映射到(0, 1)区间，从而将问题转化为概率预测。

**【逻辑回归模型】**

```math
P(y=1|x)=\frac{1}{1+e^{-\mathbf{ \omega ^{T}x}}} 
```

其中，这是一个sigmoid函数，可以输出(0,1)区间的数值，表示某一组特征值属于某一类的概率，$`\omega`$表示的是模型的参数矩阵，$`x`$表示的是模型的特征值矩阵，而P表示的是模型在给定特征x的条件下，模型属于正类(y=1)的概率；

sigmoid函数的大致图像如下，可以看到它输出(0,1)区间的数值，适合表示概率，通过改变x的系数可以改变图像的“宽度”；

<img src="./imgs/001.png" alt="001" style="zoom: 67%;" />

**【模型损失函数】**

逻辑回归的损失函数是对数损失，也被称为**交叉熵损失**，表示为：

```math
L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right]
```

其中，$`y_{i}`$表示真实标签（0/1），$`\hat{p_{i}} `$表示模型预测样本属于正类的概率，N是样本数量；

该损失函数由似然函数推导而来，似然函数：

```math
L(\omega )=\prod_{i=1}^{N}([p(x_{i})]^{y_{i}}[1-p(x_{i})]^{1-y_{i}}) 
```

要将函数的幂进行处理，并将累乘变为累加，我们需要对函数两边取对数，得到：

```math
L(\omega )=\sum_{i=1}^{N} [y_{i}logp(x_{i})+(1-y_{i})log(1-p(x_{i}))]
```

**求似然函数的最大值即是求损失函数的最小值**，所以给其添加负号，同时防止累加值过大，将其除以N，我们就能得到损失函数：

```math
J(\omega)=-\frac{1}{N}logL(\omega)= -\frac{1}{N} \sum_{i=1}^{N} [y_{i}logp(x_{i})+(1-y_{i})log(1-p(x_{i}))]
```

 

## 梯度下降法求解(纯手打公式🥵给个star支持一下主播)

类似于线性回归中的梯度下降法(可见[线性回归公式推导](./W1_T2_线性回归公式推导.md))，逻辑回归的梯度下降法也是通过求损失函数关于参数$`\omega`$的一阶偏导数寻找下降方向，从而更新模型参数，梯度计算为：

```math
\mathbf{grad} =\frac{\partial J(\omega )}{\partial \omega} =-\frac{1}{N}\sum [\frac{y_{i}}{p(x_{i})}·\frac{\partial p(x_{i})}{\partial \omega}-\frac{1-y_{i}}{1-p(x_{i})}·\frac{\partial p(x_{i})}{\partial \omega }  ] 
```

```math
=-\frac{1}{N} \sum \frac{\partial p(x_{i})}{\partial \omega }·[\frac{y_{i}}{p(x_{i})}-\frac{1-y_{i}}{1-p(x_{i})}]
```

```math
=-\frac{1}{N} \sum [p(x_{i})·(1-p(x_{i}))]·[\frac{y_{i}}{p(x_{i})}-\frac{1-y_{i}}{1-p(x_{i})}]
```

```math
化简得到：\mathbf{grad=\frac{\sum_{i=0}^{N} (p(x_{i})-y_{i})}{N} } 
```

那么通过对参数$`\omega `$进行迭代就能得到：

```math
\omega_{i}^{k+1}= \omega_{i}^{k}-\alpha ·\mathbf{grad} 
```

同样的，其中$`\alpha`$表示的是模型的学习率，k表示的是迭代的次数

## 代码实现